
@node EFFECTS NOTES
@chapter EFFECTS NOTES

Most effects in Cinelerra can be figured out just by using them and
tweaking.  Here are brief descriptions of effects which you might not
utilize fully by mere experimentation.


@menu
* 1080 TO 480::       How to convert HDTV into SD
* BACKGROUND::        Replacing transparency with rendered backgrounds
* CHROMA KEYERS::     Create transparency based on color.
* COLOR SWATCH::      Color swatch
* COMPRESSOR::        How to reduce the dynamic range of audio.
* MULTIBAND COMPRESSOR:: Reduce the dynamic range of audio by frequency range.
* DECIMATE::          How to reduce frame rates by eliminating similar frames.
* DEINTERLACE::       How to convert interlaced video to progressive video.
* DIFFERENCE KEY::    Create transparency based on color differences.
* FIELDS TO FRAMES::  How to recover interlaced video from bobbed video
* FREEZE FRAME::      How to stop action in the timeline.
* HISTOGRAM::         How to change the mapping of different brightness values.
* INVERSE TELECINE::  How to convert pulled down frames to progressive frames.
* INTERPOLATE VIDEO:: How to create the illusion of higher framerates.
* LENS::              Correcting spherical aberration
* LINEARIZE::         Fix gamma in raw camera images
* LIVE AUDIO::        Pass audio from the soundcard directly to the timeline.
* LIVE VIDEO::        Pass video from the capture card directly to the timeline.
* LOOP::              How to loop regions of the timeline.
* LUT::               Color processing with look up tables
* MOTION EFFECTS::    Stabilizing & tracking motion
* REFRAMERT::         Changing the number of frames in a sequence.
* REFRAME::           Changing the number of frames in a sequence with rendering.
* RESAMPLE::          Change the number of samples in a sequence with rendering.
* REVERSE VIDEO/AUDIO:: How to play regions in reverse.
* SWAP FRAMES::       Fixing temporal field order
* THRESHOLD::         How to get monochrome out of a region of the image.
* TIME AVERAGE::      How to stack images.
* TITLER::            How to add text to a track from inside Cinelerra.
* VIDEO SCOPE::       How to view the dynamic range of intensity and hue.
@end menu




@node 1080 TO 480
@section 1080 TO 480


Most TV broadcasts are recieved with a 1920x1080 resolution but
originate from a 720x480 source at the studio.  It's a waste of space
to compress the entire 1920x1080 if the only resolvable details are
720x480.  Unfortunately resizing 1920x1080 video to 720x480 isn't as
simple as shrinking it.

At the TV station the original 720x480 footage was first converted to
fields of 720x240.  Each field was then scaled up to 1920x540.  The two
1920x540 fields were finally combined with interlacing to form the
1920x1080 image.  This technique allows a consumer TV to display the
resampled image without extra circuitry to handle 720x480 interlacing
in a 1920x1080 image.

If you merely deinterlaced the 1920x1080 images, you would end up with
resolution of 720x240.  The @b{1080 to 480} effect properly extracts
two 1920x540 size fields from the image, resizes them separately, and
combines them again to restore a 1920x480 interlaced image.  The
@b{scale} effect must then be applied to reduce the horizontal size to
960 or 720 depending on the original aspect ratio.

The tracks to which @b{1080 to 480} is applied need to be at 1920x1080
resolution.  The project settings in @b{settings->format} should be at
least 720x480 resolution.

The effect doesn't know if the first row in the 1920x1080 image belongs
to the first row of the 720x480 original.  You have to specify what the
first row is in the effect configuration.

The output of this effect is a small image in the middle of the
original 1920x1080 frame.  Use the projector to center the output image
in the playback.

Finally, once you have 720x480 interlaced video you can either apply
@b{frames to fields} of @b{inverse telecine} to further recover original
progressive frames.

@node BACKGROUND
@section BACKGROUND

This creates either a flat color or a color cube for use with the LUT
(@xref{LUT}) effect.  Use it to fill transparent areas with a custom
color.  The color cube option is drawn in a fixed part of the frame to
aid in creating a LUT.





@include chroma_key.texi


@node COLOR SWATCH
@section COLOR SWATCH

This draws the entire color space for a fixed saturation or brightness. 
It's mainly used for testing chroma keying by providing an input to
chroma keying.  

@b{constant brightness} draws the color space for a fixed brightness. 
The brightness slider changes the brightness.

@b{constant saturation} draws the color space for a fixed saturation. 
The saturation slider changes the saturation.

@b{angle} rotate the color space to line up with another document or
interface.  

@b{draw source} draws the pixels in the source in the swatch with the
brightness or saturation they had in the source.  This shows how much of
the source is being taken out by a chroma key & where the source occurs
in the color space.



@node COMPRESSOR
@section COMPRESSOR

Contrary to computer science experience, the audio compressor does not
reduce the amount of data required to store the audio.  The audio
compressor reduces the dynamic range of the audio.  In Cinelerra the
compressor actually performs the function of an expander and
compressor.

The compressor works by calculating the maximum sound level within a
certain time period of the current position.  The maximum sound level
is taken as the input sound level.  For every input sound level there
is an output sound level specified by the user.  The gain at the
current position is adjusted so the maximum sound level in the time
range is the user specified value.

The compressor has a graph which correlates every input sound level to
an output level.  The horizontal direction is the input sound level in
dB.  The vertical direction is the ouptut sound level in dB.  The user
specifies output sound levels by creating points on the graph.  Click in
the graph to create a point.  If 2 points exist, drag one point across
another point to delete it.  The most recent point selected has its
vales displayed in @b{OUTPUT} @b{INPUT} textboxes for more precise
adjustment.

Right click in the graph to get a context menu.  

@b{copy graph} copies the graph to the clipboard.  It may be pasted in a
text editor for editing & copied from the text editor.

@b{past graph} to paste the clipboard in the graph.  This allows graphs
to be copied between band in the multiband compressor.

@b{clear graph} erases all the points.

@center @image{compressor1}
@center Graph for reducing dynamic range to 0db for all input above -42db.


@center @image{compressor2}
@center Graph for expanding dynamic range to 2db output for every 1db input.

To make the compressor reduce the dynamic range of the audio, make all
the output values greater than the input values except 0 db.  To make
the compressor expand the dynamic range of the audio, make all the
output values except 0 db less than the input values.  The algorithm
currently limits all sound levels above 0 db to 0 db so to get an
overloaded effect put a gain effect before the compressor to reduce all
the levels and follow it with another gain effect to amplify all the
levels back over 0 db.

@b{Reaction secs:} This determines where in relation to the current
position the maximum sound level is taken and how fast the gain is
adjusted to reach that peak.  It's notated in seconds.  If it's
negative the compressor reads ahead of the current position to get the
future peak.  The gain is ramped to that peak over one reaction time. 
This allows it to hit the desired output level exactly when the input
peak occurs at the current position.

If the reaction time is positive the compressor scans only the current
position for the gain and ramps gain over one reaction time to hit the
desired output level.  It hits the output level exactly one reaction
time after detecting the input peak.

@b{Decay secs:} If the peak is higher than the current level, the
compressor ramps the gain up to the peak value.  Then if a future peak
is less than the current peak it ramps the gain down.  The time taken
to ramp the gain down can be greater than the time taken to ramp the
gain up.  This ramping down time is the decay seconds.

@b{Trigger type:}  The compressor is a multichannel effect.  Several
tracks can share one compressor.  How the signal from many tracks is
interpreted is determined by the trigger type.

The @b{Trigger} trigger type uses the value supplied in the @b{Trigger}
textbox as the number of the track to use as input for the compressor. 
This allows a track which isn't even heard to determine the loudness of
the other tracks.

The @b{Maximum} trigger takes the loudest track and uses it as the
input for the compressor.

The @b{Total} trigger type adds the signals from all the tracks and
uses the total as the input for the compressor.  This is the most
natural sounding compression and is ideal when multiple tracks are
averaged into single speakers.



@b{Trigger:} The compressor is a multichannel effect.  Several tracks
can share one compressor.  Normally only one track is scanned for the
input peak.  This track is specified by the @b{Trigger}.  By sharing
several tracks and playing with the trigger value, you can make a sine
wave on one track follow the amplitude of a drum on another track for
example.

@b{Smooth only:} For visualizing what the compressor is doing to the
soundlevel, this option causes it to replace the soundwave with just
the current peak value.  It makes it very easy to see how @b{reaction
secs} affects the detected peak values.


@b{IN:} This meter shows the current value of the trigger during playback.  

@b{GAIN:} This meter shows what gain is being computed from the
trigger.  If it's above 0, the input is being amplified.  If it's below
0, the input is being reduced.


@node MULTIBAND COMPRESSOR
@section MULTIBAND COMPRESSOR

This adds more features to the compressor for selecting frequency
bands.  There are 3 bands, each with its own sound level graph.  Use the
context menu in the graph to copy curves from 1 band to another or a
text editor.

The same attack, release, trigger type, trigger, smooth only, window
size apply to all 3 bands.

@b{CURRENT BAND} Select the current band to show in the graph.

@b{SOLO BAND} Mutes the other bands & only plays the band selected.  The
solo band for the other bands is automatically deselected when this is
selected.

@b{BYPASS BAND} Disregards the graph for the current band & passes the
audio through.  The frequency range is still applied.

@b{FREQ RANGE} The multiband compressor has 3 frequency ranges which may
not overlap.  The frequency range shows 2 of the 3 border frequencies at
a time.  

For band 1, the frequency range shows only the maximum frequency of the
low range. 

For band 2, the frequency range shows the maximum frequency of the low
range & the minimum frequency of the high range.

For band 3, the frequency range shows the minimum frequency of the high range.

@b{BANDWIDTH} This is only a visual representation of the 3 bands & the
spectrum of the current audio.  It doesn't take any inputs.

@b{STEEPNESS} This is how steep the transitions between the bands are.  

@b{WINDOW SIZE} This determines the size of the FFT that creates the
frequency bands.  The higher it is, the more precise the filter but the
more it has to read ahead.  Higher steepness requires a higher window
size to truly be realized.

@node DECIMATE
@section DECIMATE

This effect drops frames from a track which are most similar in order
to reduce the frame rate.  This is usually applied to a DVD to convert
the 29.97 fps video to the 23.97 fps film rate but this decimate effect
can take any input rate and convert it to any lower output rate.

The output rate of @b{decimate} is the project frame rate.  The input
rate is set in the @b{decimate} user interface.  To convert 29.97fps
progressive video to 23.97fps film, apply a decimate effect to the
track.  Set the decimate input rate to 29.97 and the project rate to
23.97.

Be aware every effect layered before decimate processes video at the
decimate input rate and every effect layered after decimate processes
video at the project frame rate.  Computationally intensive effects
should come below decimate.








@node DEINTERLACE
@section DEINTERLACE

The deinterlace effect has evolved over the years to deinterlacing and
a whole lot more.  In fact two of the deinterlacing methods, @b{Inverse
Telecine} and @b{Frames to Fields}, are separate effects.  The
deinterlace effect offers several variations of line replication to
eliminate comb artifacts in interlaced video.  It also has some line
swapping tools to fix improperly captured video or make the result of a
reverse effect display fields in the right order.








@node DIFFERENCE KEY
@section DIFFERENCE KEY

The differency key creates transparency in areas which are similar
between 2 frames.  The Difference key effect must be applied to 2
tracks.  One track contains the action in front of a constant
background and another track contains the background with nothing in
front of it.  Apply the difference key to the track with the action and
apply a shared copy of it to the track with the background.  The track
with the background should be muted and underneath the track with the
action and the colormodel should have an alpha channel.

Pixels which are different between the background and action track are
treated as opaque.  Pixels which are similar are treated as
transparent.  Change @b{threshold} in the differency key window to make
more pixels which aren't the same color transparent. Change @b{slope}
to change the rate at which the transparency tapers off as pixels get
more different.

The slope as defined here is the number of extra values flanking the
threshold required to go from opaque to transparent.  A high slope is
more useful with a low threshold because slope fills in extra
threshold.

@b{Use value} causes the intensity of pixels to be compared instead of
the color.

Applying a blur to the top track with just the alpha channel blurred
can soften the transparency border.





@node FIELDS TO FRAMES
@section FIELDS TO FRAMES

This effects reads frames at twice the project framerate, combining 2
input frames into a single interlaced output frame.  Effects preceeding
@b{fields to frames} process frames at twice the project frame rate. 
Each input frame is called a field.

@b{Fields to frames} needs to know what field corresponds to what lines
in the output frame.  The easiest way to figure it out is to try both
options in the window.  If the input fields are the result of a line
doubling process like @b{frames to fields}, the wrong setting results
in blurrier output.  If the input fields are the result of a standards
conversion process like @b{1080 to 480}, the wrong setting won't make
any difference.

The debobber which converts 720x480 interlaced into 1920x1080
interlaced or 1280x720 progressive seems to degrade the vertical
resolution to the point that it can't be recovered.







@node FREEZE FRAME
@section FREEZE FRAME

In its simplest form, highlight a region of the track to freeze, drop
the freeze frame effect on the highlighted region, and the lowest
numbered frame in the affected area will play throughout the entire
region.  

Freezeframe has an @b{enabled} option which can be keyframed.  Regions
of a freeze frame effect which are enabled repeat the lowest numbered
frame since the last keyframe.  This has unique possibilities.

If a freeze frame effect has a keyframe in the middle of it set to
@b{enabled}, the frame in the middle is repeated in the entire effect.

If a freeze frame effect has several keyframes, each set to
@b{enabled}, every time a keyframe is encountered the frame under it
becomes the frozen one.

If a freeze frame effect alternates between @b{enabled} and
@b{disabled}, each time an @b{enabled} keyframe is encountered the
frame under it is replicated until the next @b{disabled} keyframe.  The
disabled regions play through.






@node HISTOGRAM
@section HISTOGRAM


This shows the number of occurances of each color on a histogram plot.

It is always performed in floating point RGB regardless of
the project colorspace.  The histogram has two sets of transfer
parameters: the input transfer and the output transfer.

4 histograms are possible in the histogram viewer.  The red, green,
blue histograms show the input histograms for red, green, blue and
multiply them by an input transfer to get the output red, green, blue. 
Then the output red, green, blue is scaled by an output transfer.  The
scaled red, green, blue is converted into a value and plotted on the
value histogram.  The value histogram thus changes depending on the
settings for red, green, blue.  The value transfers are applied
uniformly to R, G, B after their color transfers are applied.

Select which transfer to view by selecting one of the channels on the
top of the histogram.


The input transfer is defined by a graph overlaid on the histogram. 
The horizontal direction corresponds to every possible input color. 
The vertical direction corresponds to the output color for every input
color.  Video entering the histogram is first plotted on the histogram
plot, then it is translated so output values now equal the output
values for each input value on the input graph.

The input graph is edited by adding and removing any number of points. 
Click and drag anywhere in the input graph to create a point and move
it.  Click on an existing point to make it active and move it.  The
active point is always indicated by being filled in.  The active
point's input and output color are given in text boxes on top of the
window.  The input and output color of the point can be changed through
these text boxes.

Points can be deleted by first selecting a point and then dragging it
to the other side of an adjacent point.  They can also be deleted by
selecting them and hitting @b{delete}.


After the input transfer, the image is processed by the output
transfer.  The output transfer is simply a minimum and maximum to scale
the input colors to.  Input values of 100% are scaled down to the
output's maximum.  Input values of 0% are scaled up to the output
minimum.


Input values below 0 are always clamped to 0 and input values above
100% are always clamped to 100%.  Click and drag on the output
gradient's triangles to change it.  It also has textboxes to enter
values into.

Enable the @b{automatic} toggle to have the histogram calculate an
automatic input transfer for the red, green, blue but not the value. 
It does this by scaling the middle 99% of the pixels to take 100% of
the histogram width.  The number of pixels permitted to pass through is
set by the @b{Threshold} textbox.  A threshold of 0.99 scales the input
so 99% of the pixels pass through.  Smaller thresholds permit fewer
pixels to pass through and make the output look more contrasty.

Automatic input transfer is calculated for the R, G, and B channels but
not the value.


@b{PLOT HISTOGRAM}

@b{SPLIT OUTPUT}








@node INVERSE TELECINE
@section INVERSE TELECINE

This is the most effective deinterlacing tool when the footage is a
video transfer of a film.  Here the film was converted from 24fps to
60fps.  Then the 60fps was downsampled to 30fps by extracting odd and
even lines and interlacing the lines.  The IVTC effect is primarily a
way to convert interlaced video to progressive video.  It undoes three
patterns of interlacing.

@example
  A AB BC CD D
  AB CD CD DE EF
  Automatic
@end example

The first two options are fixed patterns and affected by the @b{pattern
offset} and @b{odd field first} parameters.  The last option creates
several combinations of lines for each frame and picks the most
progressive combination.  It's a brute force algorithm.

This technique doesn't rely on a pattern like other techniques and is
less destructive but the timing is going to be jittery because of the
lack of a frame rate reduction.  In order to smooth out the timing, you
need to follow inverse telecine with a decimate effect.





@node INTERPOLATE VIDEO
@section INTERPOLATE VIDEO


The interpolate video effect tries to create the illusion of a higher
frame rate from source footage of very low framerates by averaging
frames over time.  It averages two input frames for each output frame. 
The input frames are at different times, resulting in a dissolve for
all output frames between the input frames.  There are two ways of
specifying the input frames.  You can specify an input frame rate which
is lower than the project frame rate.  This causes input frames to be
taken at even intervals,

You can also specify keyframe locations as the positions of the input
frames.  In this mode the output frame rate is used as the input frame
rate and you just create keyframes wherever you want to specify an
input frame.


@node LENS
@section LENS

The lens affect stretches or shrinks to convert lens distorted images to
rectilinear images.  The most common use is converting fish eye lenses
to rectilinear lenses.  It is also useful for star tracking.

@b{R, G, B, A Field of view:} These determine how much the image is
stretched in each channel.

@b{Lock:} This causes changes to 1 channel to affect all the channels. 
This is normally the desired behavior.

@b{Aspect Ratio:} This changes the amount of stretching done in the X
axis vs the Y axis.  To crop less data from stretched images, this
allows more stretching to be done on 1 axis without creating black
borders in the other axis.

@b{Radius:} This determines the size of the stretched region.  While
adjusting the @b{field of view}, black borders may appear.  Adjust the
@b{radius} to shrink or expand the output so black borders are out of
frame.

@b{Center X, Y:} The center of the stretched region.  This is only
useful if the image was previously translated by the software so the
center of the lens is now off center.

@b{Draw center:} This is a visual aid when adjusting the @b{Center X, Y}
but doesn't affect the results.

@b{Mode:} The type of stretching algorithm.

@itemize
@item
@b{Sphere shrink:} This is for making an image look like it's mapped to a sphere.

@item
@b{Sphere expand:} This is for unmapping an image mapped to a sphere and
flattening it.

@item
@b{Rectilinear Stretch:} This is for flattening a fish eye lens.

@item
@b{Rectilinear Shrink:} This is for making something flat look like it
was taken by a fish eye lens.


@end itemize











@node LINEARIZE
@section LINEARIZE


Raw camera images store colors in a logarithmic scale.  The blacks in
these images are nearly 0 and the whites are supposed to be infinity. 
The graphics card and most video codecs store colors in a linear scale
but Cinelerra keeps raw camera images in their original logarithmic
scale when it renders them.  This is necessary because the raw image
parser can't always decode the proper gamma values for the images.  It
also does its processing in 16 bit integers, which takes away a lot of
information.

The linearize effect converts the logarithmic colors to linear colors
through a gamma value and a maximum value.  The gamma value determines
how steep the output curve is and the maximum value is where 1.0 in the
output corresponds to maximum brightness in the input.

The linearize effect has 2 more parameters to simplify gamma
correction.  The @b{automatic} option causes it to calculate @b{max}
from the histogram of the image.  Use this when making a preview of a
long list of images since it changes for every image.

The @b{use color picker} option uses the value currently in the color
picker to set the @b{max} value.  Note that every time you pick a color
from the compositor window, you need to hit @b{use color picker} to
apply the new value.








@node LIVE AUDIO
@section LIVE AUDIO

This effect reads audio directly from the soundcard input.  It replaces
any audio on the track so it's normally applied to an empty track. 


To use Live Audio, highlight a horizontal region of an audio track or
define in and out points.  Then drop the Live Audio effect into it. 
Create extra tracks and attach shared copies of the first Live Audio
effect to the other tracks to have extra channels recorded.

Live Audio uses the sound driver selected in
@b{Settings->Preferences->Playback->Audio Out} for recording, but
unlike recording it uses the @b{playback buffer size} as the recording
buffer size and it uses the @b{project sample rate} as the sampling
rate.

These settings are critical since some sound drivers can't record in
the same sized buffer they play back in.  Live audio has been most
reliable when ALSA is the recording driver and the playback fragment
size is 2048.

Drop other effects after Live Audio to process soundcard input in
realtime.  

Now the bad news.  With live audio there is no readahead so effects
like compressor will either delay if they have readahead enabled or
playback will underrun.  

Another problem is sometimes the recording clock on the soundcard is
slightly slower than the playback clock.  The recording eventually
falls behind and playback sounds choppy.

Finally, live audio doesn't work in reverse.





@node LIVE VIDEO
@section LIVE VIDEO

This effect reads video directly from the capture card input.  It
replaces any video on the track so it's normally applied to an empty
track.  The configuration for the capture card is taken from the
recording preferences.  Go to @b{Settings->Preferences->Recording} to
set up the capture card.

Go to the @b{Video In} section where it says @b{Record driver}.  It
must be set to either @b{Video4Linux2} or @b{IEC 61883}.  Other video
drivers haven't been tested with Live Video and probably won't work.

For live video, the selection for @b{File Format} and @b{Video} needs
to be set to a format the timeline can use.  The file format must be
@b{Quicktime for Linux} and video recording must be enabled for it. 
Click on the wrench @image{wrench} to set the video compression.

The video compression depends on the recording driver.  For the
@b{Video4Linux2} recording driver, the compression must be @b{Motion
JPEG A}.  For the @b{IEC 61883} driver, the compression must be
@b{DV}.  This gets the driver to generate output in a colormodel that
the timeline can use.

Some cards provide color and channel settings.  Live video takes the
color settings from the values set in the @b{Video In} window.  Go to
@b{File->Record} to bring up the recording interface and the Video In
window.  Values set in the @b{Video in} window are used by @b{Live
Video}.  Any channels the capture card supports need to be configured
in the @b{Video in} interface since the same channels are used by the
@b{Live Video} effect.

With the video recording configured, highlight a horizontal region of a
video track or define in and out points.  Then drop the Live Video
effect into it.  Drop other effects after Live Video to process the
live video in realtime.  For best results, you should use OpenGL and a
video card which supports GL shading language.  Go to
@b{Settings->Preferences->Playback->Video Out} to enable the OpenGL
driver.

Only one Live Video effect can exist at any time on the timeline.  It
can't be shared by more than one track.






@node LOOP
@section LOOP

Sections of audio or video can be looped by dropping a @b{loop} effect
on them.  Contrary to the the @b{settings->loop playback} option, the
loop effects can be rendered where the @b{settings->loop playback}
option can not be.  The loop effects are also convenient for short
regions.

The loop effects have one option: the number of @b{frames} or
@b{samples} to loop.  This specifies the length of the region to loop
starting from either the beginning of the effect or the latest
keyframe.  The region is replicated for the entire effect.

Every time a keyframe is set in a loop effect, the keyframe becomes the
beginning of the region to loop.  Setting several keyframes in
succession causes several regions to loop.  Setting a single keyframe
causes the region after the keyframe to be looped throughout the
effect, no matter where the keyframe is.  The end of an effect can be
looped from the beginning by setting the keyframe near the end.



@node LUT
@section LUT

This effect looks up the value of every output color in a table of all
possible input colors.  It requires an 8 bit RGB 512x512 PNG image
containing the color cube.  This is the same format ffmpeg uses to apply
a LUT.  The output color is interpolated from 2 color entries in the
table.

@b{CREATING A LUT TABLE}

To create a color cube for the LUT effect, 1st create a timeline which
performs all the color processing.  Then apply the @b{Background} effect
to render a @b{color cube} instead of the source footage.  This requires
selecting @b{color cube} as the background type.  The background renders
a fixed size color cube for the LUT.

Move the background effect to before the color processing.  This is
easiest done by right clicking on it to access the context menu.  Select
@b{move down} to make it wrap to the top of the stack.

@center @image{move_plugin}

Change the project size to 512x512.  Be sure the track size is big
enough to contain the color cube.  Change the projector to left top
justify.  Render 1 frame to a PNG image.  The color cube must fill the
entire 512x512 output & be aligned exactly.

There is no cookie cutter feature to generate a LUT because it's an
extremely rarely used feature.


@include motion.texi


@node REFRAMERT
@section REFRAMERT


ReframeRT changes number of frames in a sequence of video directly from
the timeline.  It has 2 modes, selected by the 2 toggles in the GUI.

@b{Stretch} mode multiplies the current frame number of its output by
the scale factor to arrive at the frame to read from its input.  If its
current output frame is #55 and the scale factor is 2, frame #110 is
read from its input.  The stretch mode has the effect of changing the
length of output video by the inverse of the scale factor.  If the
scale factor is greater than 1, the output will end before the end of
the sequence on the timeline.  If it's less than 1, the output will end
after the end of the sequence on the timeline.  The ReframeRT effect
must be lengthened to the necessary length to accomodate the scale
factor.  Change the length of the effect by clicking on the endpoint of
the effect and dragging.

Although stretch mode changes the number of the frame read from its
input, it doesn't change the frame rate of the input.  Effects before
ReframeRT assume the same frame rate as ReframeRT.

@b{Downsample} mode doesn't change the length of the output sequence. 
It multiplies the frame rate of the output by the scale factor to
arrive at a frame rate rate to read the input.  This has the effect of
replicating the input frames so that they only change at the scaled
frame rate when sent to the output.  It doesn't change the length of
the sequence.  If the scale factor is 0.5 and the output frame rate is
30 fps, only 15 frames will be shown per second and the input will be
read at 15 fps.  Downsample is only useful for scalefactors below 1,
hence the name downsample.

Downsample mode changes the frame rate of the input as well as the
number of the frame to read, so effects before ReframeRT see the frame
rate * the scale factor as their frame rate.  If the scale factor is 2
and the output frame rate is 30, the input frame rate will be 60 and
the input frame number will by doubled.  This won't normally do
anything but some input effects may behave differently at the higher
frame rate.




@node REFRAME
@section REFRAME

This does exactly the same thing as @b{ReframeRT} in @b{Stretch} mode. 
It multiplies the output frame number by the scale factor to arrive at
the input frame number and changes the length of the sequence.  Unlike
ReframeRT, this must run from the @b{Video} menu and render its output.

Be aware @b{Reframe} doesn't write the scaled frame rate as the frame
rate of the rendered file.  It produces a file of scaled length and
equal frame rate as the project.  The new length is 1/scale factor as
big as the original sequence.








@node RESAMPLE
@section RESAMPLE

This multiplies the number of each output sample by a scale factor to
arrive at the number of the input sample.  The output file's sample
rate is set to the project sample rate but its length is changed to
reflect the scaled number of samples.  It also filters the resampled
audio to remove aliasing.

If the scale factor is 2, every 2 input samples will be reduced to 1
output sample and the output file will have half as many samples as the
input sequence.  If it's 0.5, every 0.5 input samples will be stretched
to 1 output sample and the output file will have twice as many samples
as the input sequence.










@node REVERSE VIDEO/AUDIO
@section REVERSE VIDEO/AUDIO

Media can be reversed on the timeline in realtime.  This isn't to be
confused with using the reverse playback on the transport.  The reverse
effects reverse the region covered by the effect regardless of the
transport direction.  Apply @b{reverse audio} to an audio track and
play it backwards.  The sound plays forward.

The region to be reversed is first determined by what part of the track
the effect is under and second by the locations of keyframes in the
effect.  The reverse effects have an @b{enabled} option which allows
you to set keyframes.  This allows may possibilities.

Every @b{enabled} keyframe is treated as the start of a new reversed
region and the end of a previous reversed region.  Several @b{enabled}
keyframes in succession yield several regions reversed independant of
each other.  An @b{enabled} keyframe followed by a @b{disabled}
keyframe yields one reversed region followed by a forward region.

Finally, be aware when reversing audio that the waveform on the
timeline doesn't reflect the actual reversed output.




@node SWAP FRAMES
@section SWAP FRAMES

This is normally used on interlaced video which has been converted to
fields.  One set of lines becomes one frame and the other set of lines
becomes the next frame.  Now the frame rate is double and is showing
fields of the original video sequentially.  The fields may be out of
order, which is what @b{SWAP FRAMES} can correct.












@node THRESHOLD
@section THRESHOLD

Threshold converts the image to pure luminance.  Then luminance values
below and above the threshold range are converted to black and
luminance values inside the threshold range are converted to white. 
The threshold window shows a histogram of luminance values for the
current frame.  Click dragging inside the histogram creates a range to
convert to white.  Shift-clicking extends one border of this range. 
Values for the threshold range can also be specified in the text boxes.

This effect is basically a primitive luminance key.  A second track
above the track with the threshold effect can be multiplied, resulting
in only the parts of the second track within the threshold being
displayed.







@node TIME AVERAGE
@section TIME AVERAGE

Time average is one effect which has many uses besides creating nifty
trail patterns of moving objects.  It's main use is reducing noise in
still images.  Merely point a video camera at a stationary subject for
30 frames, capture the frames, and average them using TIME AVERAGE and
you'll have a super high quality print.  In floating point colormodels, time
average can increase the dynamic range of lousy cameras.

Inside the time average effect is an accumulation buffer and a
divisor.  A number of frames are accumulated in the accumulation buffer
and divided by the divisor to get the average.

Because the time average can consume enourmous amounts of memory, it is
best applied by first disabling playback for the track, dropping the
time average in it, configuring time average for the desired number of
frames, and re-enabling playback for the track.

@b{Frames count:} This determines the number of frames to be accumulated
in the accumulation buffer.  For extremely large integrations it's
easier to edit the EDL in a text editor and put in the number of frames.

@b{Accumulate sequence again:} If an effect before the time average is
adjusted the time average normally doesn't reread the accumulation
buffer to get the change.  This forces it to reread the accumulation
buffer when any other effects change.


@b{Average:} This causes the accumulation buffer to be divided before
being output.  The result is the average of all the frames.

@b{Accumulate:} This outputs the accumulation buffer without dividing
it.  The result is the sum of all the frames.

@b{Accumulate only:}

In order to accumulate only the specified number of frames, the time
average retains all the previous frames in memory and subtracts them out
as it plays forward.  @b{Accumulate only:} causes the history buffer to
not be used in @b{averaging} and @b{accumulating}.  Without the history
buffer, frames are added endlessly without ever being subtracted.  It's
the same as an infinitely long accumulation buffer.  The only difference
is for @b{Average} mode, the output is still divided by the @b{Frame
count}.  @b{Accumulate only} is used where the number of frames in the
accumulation would be too big to fit in memory.

@b{Replace:} This causes the accumulation buffer to be replaced by only
pixels which aren't transparent.  This allows black borders from motion
tracking to be filled in.

@b{Threshold:} The value a pixel must be before it replaces the previous
pixel.  If alpha channels are enabled, the alpha is the value compared. 
If there is no alpha channel, the brightness is the value compared.

@b{Border:} The number of pixels on the border of the image to never
replace.  This hides errors in the replacement operation from the
output, since errors occur at the transition between the replaced area
and the ignored area.

@b{Greater:} Pixels are replaced if their value is greater than the
previous pixel.  Use this to create star trails in stacks of many night
sky photos or paint many copies of an object from its motion if it is
lighter than the background.

@b{Less:} Pixels are replaced if their value is less than the previous
pixel.  Use this to paint copies of an object from its motion if it is
darker than the background.







@node TITLER
@section TITLER

While it is possible to add text to movies by importing still images
from The Gimp and compositing them, the Titler allows you to add text
from within Cinelerra.  

The titler has standard options for @b{font, size, and style}.  The
best font is a generic, normal font like Arial in a large size.

The titler also has options you'll only find in moving pictures.  The
@b{Justify} operation justifies the text relative to the entire frame. 
Once justified, the @b{X and Y} offset is applied.  This allows text to
be justified while at the same time letting you push it within the
title safe region.

The @b{motion type} scrolls the text in any of the four directions. 
When using this, the text may dissappear.  Move the insertion point
along the timeline until the text is far enough along the animation to
reappear.  The text scrolls on and scrolls off.

Setting @b{loop} causes the text to scroll completely off and repeat. 
Without @b{loop} the text scrolls off and never reappears.

The speed of the animation is determined by @b{speed}  Set it higher to
speed up the animation.

@b{Drop shadow} draws a black copy of the text to the bottom right of
the original text.  Useful when drawing text over changing video to
keep the border always visible.

In addition to the scrolling, @b{Fade in/Fade out} are a second type of
animation.  If the fade seconds are 0, no fading is done.

@b{Outline} draws an outline on the characters if it's greater than 0. 
Set the outline size by changing the number.  Set the outline color with
the @b{OUTLINE COLOR} button.  If no outline is visible, make sure the
alpha in @b{OUTLINE COLOR} is nonzero.  To get pure outline characters,
set @b{COLOR} alpha to 0.

@b{COLOR} picks the color to draw the text in.  Usually white is the
only practical color.

@b{OUTLINE COLOR} picks the color to draw the text outline in.

@b{Stamp timecode} replaces the text with the current position on the
timeline in seconds and frames.

@b{SECRETS OF THE TITLER}
@menu
* ADDING FONTS TO THE TITLER:: How to add fonts to the titler
@ignore
* THE TITLE-SAFE REGION::      How to keep text visible on output
@end ignore
* MAKING TITLES LOOK GOOD::    How to make your titles look good.
@end menu

@node ADDING FONTS TO THE TITLER
@subsection ADDING FONTS TO THE TITLER

Cinelerra uses libfreetype for drawing titles.  It has a few
limitations: it can't automatically use the fonts installed in the X
server.  It can't automatically determine the files to use for normal,
italic, bold.  Some distributions began putting all their fonts in
/usr/share/fonts but different programs still have their own directories
to guarantee they work.

The easiest way to support fonts has been to put the .ttf files in
@b{cinelerra/fonts}.  Then run @b{ttmkfdir > fonts.dir} and restart
Cinelerra.  The new fonts should appear.  The usage of ttmkfdir changes
frequently so this technique might not work.

The titler supports whatever ttmkfdir & freetype support, mainly
@b{TTF}, true type fonts.  Other font formats like OTF are best
converted to TTF.  It's not the biggest limitation of the titler.

@ignore
@node THE TITLE-SAFE REGION
@subsection THE TITLE-SAFE REGION

If the video is displayed on a consumer TV, the outer border is going
to be cropped by 5% on each side.  Moreover, text which is too close to
the edge looks sloppy.  Make sure when adding titles to have the
@b{title-safe} @image{titlesafe} tool active in the @b{compositor} window.
The text shouldn't cross the inner rectangle.
@end ignore


@node MAKING TITLES LOOK GOOD
@subsection MAKING TITLES LOOK GOOD

No-one else is going to tell you this, but to make good looking titles,
ignore most of the features of the titler.  The best settings are:

@itemize
@item
Font: @b{Arial}
@item
Italic: @b{off}
@item
Motion: @b{No motion}
@item
Bold: @b{on or off}
@item
Fade in: @b{0}
@item
Fade out: @b{0}
@item
Color: @b{white}
@item
Outline color: @b{black}
@item
Drop Shadow or outline: @b{use either to improve contrast but not both}
@end itemize


Don't waste the audience's time with fading & crawls.  Use crawls only
if there's too much text to fit on the screen.  The title should be
legible enough to take the least amount of time to read.  You're
supposed to show the story, not write it.  If they wanted to read a
story, they would be reading a book instead of watching video.














@node VIDEO SCOPE
@section VIDEO SCOPE

The video scope plots two views of the image.  One view plots the
intensity of each pixel against horizontal position.  They call this
the WAVEFORM.  Another view translates hue to angle and saturation to
radius for each pixel.  They call this the VECTORSCOPE.

The vectorscope is actually very useful for determining if an image is
saturated.  When adjusting saturation, it's important to watch the
vectorscope to make sure pixels don't extend past the 100 radius.

The waveform allows you to make sure image data extends from complete
black to complete white while adjusting the brightness/contrast.

Some thought is being given to having a video scope for recording. 
Unfortunately, this would require a lot of variations of the video
scope for all the different video drivers.

